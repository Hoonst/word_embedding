{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('cs224w')",
   "metadata": {
    "interpreter": {
     "hash": "11fe729db4f79aed458297f4cec3c3629ef4e04793785c93ae28376e0154d3b6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def preprocess(text):\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace(\".\", \" <PERIOD> \")\n",
    "    text = text.replace(\",\", \" <COMMA> \")\n",
    "    text = text.replace('\"', \" <QUOTATION_MARK> \")\n",
    "    text = text.replace(\";\", \" <SEMICOLON> \")\n",
    "    text = text.replace(\"!\", \" <EXCLAMATION_MARK> \")\n",
    "    text = text.replace(\"?\", \" <QUESTION_MARK> \")\n",
    "    text = text.replace(\"(\", \" <LEFT_PAREN> \")\n",
    "    text = text.replace(\")\", \" <RIGHT_PAREN> \")\n",
    "    text = text.replace(\"--\", \" <HYPHENS> \")\n",
    "    text = text.replace(\"?\", \" <QUESTION_MARK> \")\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(\":\", \" <COLON> \")\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove all words with  5 or fewer occurences\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > 5]\n",
    "\n",
    "    return trimmed_words\n",
    "\n",
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: Two dictionaries, vocab_to_int, int_to_vocab\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "    def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = words[start:idx] + words[idx+1:stop+1]\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "words = preprocess(text)\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "#print(list(word_counts.items())[0])  # dictionary of int_words, how many times they appear\n",
    "\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total words in text: 16680599\nUnique words: 63641\n"
     ]
    }
   ],
   "source": [
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580]\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "print(int_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[5233, 3080, 58, 10571, 27349, 15067, 58112, 150, 3580, 10712, 454, 6, 7088, 5233, 10, 44611, 2877, 2621, 8983, 6437, 4186, 5233, 447, 4860, 6753, 7573, 1774, 566, 11064, 7088]\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "#print(list(word_counts.items())[0])  # dictionary of int_words, how many times they appear\n",
    "\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "\n",
    "print(train_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "words length has shrinked from 16680599 -> 4628728\n"
     ]
    }
   ],
   "source": [
    "print(f'words length has shrinked from {len(words)} -> {len(train_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16680599"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "len(int_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nTarget:  [2, 3, 4, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# test your code!\n",
    "\n",
    "# run this cell multiple times to check for random window selection\n",
    "int_text = [i for i in range(10)]\n",
    "print('Input: ', int_text)\n",
    "idx=5 # word index of interest\n",
    "\n",
    "target = get_target(int_text, idx=idx, window_size=5)\n",
    "print('Target: ', target)  # you should get some indices around the idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x\n [0, 1, 1, 1, 2, 2, 2, 3]\ny\n [1, 0, 2, 3, 0, 1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "int_text = [i for i in range(20)]\n",
    "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
    "\n",
    "print('x\\n', x)\n",
    "print('y\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "        Here, embedding should be a PyTorch embedding module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        \n",
    "        # define embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab,n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab,n_embed)\n",
    "        \n",
    "        # Initialize both embedding tables with uniform distribution\n",
    "        self.in_embed.weight.data.uniform_(-1,1)\n",
    "        self.out_embed.weight.data.uniform_(-1,1)\n",
    "        \n",
    "    def forward_input(self, input_words):\n",
    "        # return input vector embeddings\n",
    "        input_vector = self.in_embed(input_words)\n",
    "        return input_vector\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        # return output vector embeddings\n",
    "        output_vector = self.out_embed(output_words)\n",
    "\n",
    "        return output_vector\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        \"\"\" Generate noise vectors with shape (batch_size, n_samples, n_embed)\"\"\"\n",
    "        if self.noise_dist is None:\n",
    "            # Sample words uniformly\n",
    "            noise_dist = torch.ones(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        # Sample words from our noise distribution\n",
    "        noise_words = torch.multinomial(noise_dist,\n",
    "                                        batch_size * n_samples,\n",
    "                                        replacement=True)\n",
    "        \n",
    "        device = \"cuda\" if model.out_embed.weight.is_cuda else \"cpu\"\n",
    "        noise_words = noise_words.to(device)\n",
    "        \n",
    "        ## TODO: get the noise embeddings\n",
    "        # reshape the embeddings so that they have dims (batch_size, n_samples, n_embed)\n",
    "        # as we are adding the noise to the output, so we will create the noise vectr using the\n",
    "        # output embedding layer\n",
    "        noise_vector = self.out_embed(noise_words).view(batch_size,n_samples,self.n_embed)        \n",
    "        return noise_vector\n",
    "        \n",
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        \n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        #debugging\n",
    "        #print(type(noise_vectors)) #it is a tensor\n",
    "        \n",
    "        #'neg' returns the negative of a tensor\n",
    "        #print(noise_vectors)\n",
    "        #print(noise_vectors.neg())\n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
    "\n",
    "        # negate and sum correct and noisy log-sigmoid losses\n",
    "        # return average batch loss\n",
    "        return -(out_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " were, joe, vector\n",
      "articles | geocities, whoso, synonymously, of, pechenegs\n",
      "ice | devastated, deirdre, packs, enz, testimony\n",
      "ocean | the, cidade, steiner, shine, stag\n",
      "...\n",
      "\n",
      "Epoch: 1/5\n",
      "Loss:  4.144178867340088\n",
      "were | as, the, he, to, in\n",
      "its | of, and, from, the, which\n",
      "three | five, zero, eight, two, four\n",
      "are | a, of, is, that, the\n",
      "five | three, two, eight, six, four\n",
      "used | a, and, are, not, to\n",
      "two | zero, five, three, eight, one\n",
      "in | the, of, and, a, to\n",
      "report | mooney, hindrance, housekeeping, baikonur, winding\n",
      "shows | closer, nim, noises, scythians, legion\n",
      "award | his, american, appointment, prize, verses\n",
      "brother | jehu, town, kennel, film, sqq\n",
      "applied | albright, figures, lo, from, among\n",
      "joseph | z, it, voices, licensed, people\n",
      "smith | consultancies, delicacy, tropic, hodgson, jaroslav\n",
      "quite | disagree, named, locust, from, all\n",
      "...\n",
      "\n",
      "Epoch: 1/5\n",
      "Loss:  3.4501428604125977\n",
      "but | as, that, the, to, a\n",
      "united | the, states, kingdom, after, government\n",
      "one | seven, eight, nine, five, six\n",
      "with | a, to, are, of, that\n",
      "during | of, by, was, the, after\n",
      "while | which, of, to, from, on\n",
      "world | of, their, in, were, was\n",
      "are | with, can, is, it, be\n",
      "discovered | celluloid, henri, derrick, selig, tonsure\n",
      "versions | windows, common, be, character, system\n",
      "road | overnight, meet, cassava, msn, cesare\n",
      "quite | not, modern, view, that, compressor\n",
      "institute | science, today, organizes, photographing, ordinary\n",
      "troops | minority, were, union, after, army\n",
      "nobel | american, seven, initiator, cree, deplored\n",
      "file | system, images, based, software, error\n",
      "...\n",
      "\n",
      "Epoch: 1/5\n",
      "Loss:  3.2843728065490723\n",
      "has | the, and, for, in, including\n",
      "state | united, states, party, international, president\n",
      "th | century, empire, and, by, the\n",
      "its | the, of, on, and, in\n",
      "use | can, a, it, used, the\n",
      "all | a, the, on, in, to\n",
      "used | is, example, or, be, also\n",
      "during | first, had, war, the, and\n",
      "applied | referred, because, zsef, true, case\n",
      "experience | win, blues, but, being, ctrl\n",
      "lived | in, began, particularly, people, kingdom\n",
      "rise | by, established, land, bc, which\n",
      "except | is, or, it, if, be\n",
      "grand | charles, pushkin, arthur, british, cellist\n",
      "proposed | legal, rapidly, they, the, but\n",
      "operating | software, system, unix, windows, interface\n",
      "...\n",
      "\n",
      "Epoch: 1/5\n",
      "Loss:  3.4827914237976074\n",
      "years | age, total, population, female, est\n",
      "an | a, with, the, as, to\n",
      "some | a, and, many, common, the\n",
      "history | in, became, also, links, new\n",
      "is | a, are, the, of, or\n",
      "world | the, became, in, foreign, war\n",
      "in | the, of, also, and, first\n",
      "states | council, government, nations, union, national\n",
      "units | unit, defined, per, equal, limit\n",
      "report | news, public, org, resolution, depending\n",
      "creation | create, in, support, history, project\n",
      "frac | sum, y, infty, f, x\n",
      "pope | emperor, brother, king, reign, louis\n",
      "question | knowledge, you, that, what, answer\n",
      "pre | with, the, a, in, more\n",
      "channel | channels, network, stations, show, radio\n",
      "...\n",
      "\n",
      "Epoch: 2/5\n",
      "Loss:  2.742795705795288\n",
      "some | as, the, many, other, that\n",
      "world | home, major, became, s, western\n",
      "in | to, and, the, on, also\n",
      "between | the, of, from, which, is\n",
      "many | some, these, and, the, of\n",
      "its | in, has, to, and, than\n",
      "this | the, or, can, and, not\n",
      "has | and, the, a, its, of\n",
      "animals | species, animal, human, humans, cannot\n",
      "ocean | atlantic, sea, pacific, island, coast\n",
      "taking | a, and, the, in, invasion\n",
      "operating | software, system, hardware, windows, os\n",
      "rise | its, middle, mountains, eastern, river\n",
      "versions | version, interface, software, display, windows\n",
      "recorded | movie, album, hit, pangloss, blues\n",
      "behind | back, crew, off, to, down\n",
      "...\n",
      "\n",
      "Epoch: 2/5\n",
      "Loss:  2.9120430946350098\n",
      "all | just, to, than, more, or\n",
      "at | the, in, three, by, to\n",
      "has | as, other, of, that, by\n",
      "their | to, would, them, against, they\n",
      "six | four, five, seven, one, two\n",
      "but | a, to, then, so, or\n",
      "his | who, he, father, himself, him\n",
      "as | in, the, for, a, to\n",
      "issue | justice, court, debate, reforms, that\n",
      "older | age, females, size, household, median\n",
      "additional | program, versions, designed, requires, allow\n",
      "pre | large, a, as, middle, larger\n",
      "mathematics | mathematical, algebra, theorem, theory, physics\n",
      "mean | is, define, f, continuous, n\n",
      "experience | psychological, spiritual, focus, while, may\n",
      "grand | poland, empire, independence, italy, became\n",
      "...\n",
      "\n",
      "Epoch: 2/5\n",
      "Loss:  2.6446216106414795\n",
      "would | him, was, and, the, he\n",
      "who | him, his, he, had, himself\n",
      "these | are, types, be, well, used\n",
      "b | d, f, one, c, l\n",
      "there | is, but, of, are, has\n",
      "when | or, therefore, thus, cannot, but\n",
      "often | or, are, well, be, this\n",
      "in | the, and, of, a, to\n",
      "proposed | process, of, by, theory, the\n",
      "square | km, circle, map, points, diameter\n",
      "pre | of, and, the, traditionally, around\n",
      "question | truth, should, ask, we, this\n",
      "notes | articles, books, article, pp, works\n",
      "joseph | born, nine, richard, politician, one\n",
      "troops | forces, invasion, army, armies, military\n",
      "event | universe, earth, sun, horizon, observed\n",
      "...\n",
      "\n",
      "Epoch: 2/5\n",
      "Loss:  2.590472459793091\n",
      "however | the, in, of, between, not\n",
      "of | the, and, as, in, from\n",
      "were | the, had, been, during, of\n",
      "he | his, him, himself, father, her\n",
      "three | one, four, six, five, seven\n",
      "but | not, the, causing, to, into\n",
      "that | be, what, this, is, not\n",
      "d | n, c, b, l, j\n",
      "active | activity, materials, for, some, information\n",
      "road | roads, traffic, city, route, street\n",
      "bbc | tv, news, uk, television, articles\n",
      "woman | her, female, birth, mother, children\n",
      "ocean | atlantic, pacific, coast, mountains, islands\n",
      "scale | observations, vast, farming, much, fossil\n",
      "derived | meaning, similar, used, written, word\n",
      "writers | poets, stories, literary, fiction, literature\n",
      "...\n",
      "\n",
      "Epoch: 2/5\n",
      "Loss:  2.538872003555298\n",
      "four | zero, five, three, one, seven\n",
      "used | or, commonly, use, standard, uses\n",
      "were | been, was, the, remained, had\n",
      "some | non, is, other, of, has\n",
      "at | a, and, with, in, following\n",
      "united | states, state, international, national, union\n",
      "people | living, china, rural, minority, ethnic\n",
      "is | of, this, are, or, the\n",
      "nobel | prize, laureate, physicist, b, recipient\n",
      "numerous | many, was, several, were, ancient\n",
      "mathematics | mathematical, theory, philosophy, algebra, theorem\n",
      "pressure | oxygen, liquid, temperature, gas, increases\n",
      "road | city, nearby, roads, railway, river\n",
      "paris | french, la, france, le, de\n",
      "universe | cosmic, explained, motion, nature, celestial\n",
      "taking | his, once, justice, did, asked\n",
      "...\n",
      "\n",
      "Epoch: 2/5\n",
      "Loss:  2.8391711711883545\n",
      "up | the, than, with, enough, once\n",
      "people | living, to, majority, peoples, ethnic\n",
      "years | total, population, female, birth, age\n",
      "four | five, one, six, zero, three\n",
      "such | other, many, most, some, various\n",
      "new | london, york, opened, home, metropolitan\n",
      "war | allied, forces, battle, troops, nazi\n",
      "nine | one, four, seven, eight, six\n",
      "freedom | liberty, socialists, congress, liberal, rights\n",
      "gold | silver, bronze, copper, green, metals\n",
      "troops | forces, army, soldiers, fought, war\n",
      "event | championship, events, win, wins, tournament\n",
      "question | answer, what, there, we, think\n",
      "resources | comprehensive, resource, communication, management, source\n",
      "hit | record, hits, album, pitcher, fans\n",
      "derived | using, combination, word, uses, analogous\n",
      "...\n",
      "\n",
      "Epoch: 3/5\n",
      "Loss:  2.6310112476348877\n",
      "will | that, if, when, must, any\n",
      "which | is, these, very, and, with\n",
      "their | they, to, were, them, others\n",
      "called | is, forms, where, form, into\n",
      "so | or, order, reason, cannot, that\n",
      "see | also, an, external, references, on\n",
      "and | the, in, of, s, to\n",
      "american | actor, actress, musician, comedian, canadian\n",
      "defense | target, bombers, defensive, personnel, pilots\n",
      "primarily | most, predominantly, largely, include, primary\n",
      "pope | vi, xii, emperor, iv, constantinople\n",
      "units | unit, ratio, per, weights, measured\n",
      "frac | sqrt, infty, cdot, x, mathbf\n",
      "mainly | regional, and, indigenous, ethnic, of\n",
      "additional | each, add, versions, a, screenshots\n",
      "woman | her, birth, marriage, female, child\n",
      "...\n",
      "\n",
      "Epoch: 3/5\n",
      "Loss:  2.4638772010803223\n",
      "time | in, the, on, then, a\n",
      "world | history, international, sports, africa, and\n",
      "when | to, after, up, a, that\n",
      "was | after, had, were, in, the\n",
      "used | common, use, these, uses, using\n",
      "would | to, so, that, never, anything\n",
      "it | which, that, be, but, is\n",
      "their | they, to, once, the, losing\n",
      "pre | see, traditionally, are, western, terminology\n",
      "behind | back, team, yards, off, up\n",
      "marriage | her, married, marry, parents, woman\n",
      "instance | defined, g, define, is, be\n",
      "primarily | primary, largely, more, as, include\n",
      "powers | governor, constitution, government, senate, council\n",
      "quite | still, than, attention, styles, characterized\n",
      "event | racing, events, caused, winter, observed\n",
      "...\n",
      "\n",
      "Epoch: 3/5\n",
      "Loss:  2.5746288299560547\n",
      "were | had, been, who, have, their\n",
      "their | they, would, and, them, more\n",
      "used | use, or, different, form, common\n",
      "world | religious, america, also, see, influence\n",
      "are | these, different, is, well, can\n",
      "people | speak, ethnic, native, americans, tribes\n",
      "with | and, the, by, of, to\n",
      "they | because, or, are, them, their\n",
      "paris | de, sculptor, sur, du, le\n",
      "engineering | engineers, technology, systems, research, science\n",
      "creation | genesis, created, science, beings, nature\n",
      "pre | features, history, term, today, significantly\n",
      "older | have, versus, females, less, age\n",
      "placed | head, required, hand, a, wound\n",
      "governor | appointed, executive, legislature, unsuccessfully, chief\n",
      "writers | poets, fiction, novelists, philosophers, literature\n",
      "...\n",
      "\n",
      "Epoch: 3/5\n",
      "Loss:  2.4130804538726807\n",
      "were | during, had, invasion, and, the\n",
      "one | seven, two, four, five, six\n",
      "war | troops, army, forces, civil, fought\n",
      "other | some, is, of, all, and\n",
      "be | can, not, are, that, what\n",
      "where | are, or, mostly, n, is\n",
      "six | one, two, three, four, five\n",
      "and | the, of, by, in, to\n",
      "shows | television, broadcasting, show, tv, shown\n",
      "creation | genesis, created, human, concept, original\n",
      "versions | version, windows, desktop, microsoft, operating\n",
      "ice | glaciers, dry, rocks, glacial, melting\n",
      "http | www, org, html, links, com\n",
      "channel | channels, broadcasting, broadcast, broadcasts, radio\n",
      "pope | vi, archbishop, xii, rome, church\n",
      "animals | mammals, species, humans, animal, wild\n",
      "...\n",
      "\n",
      "Epoch: 3/5\n",
      "Loss:  2.520909070968628\n",
      "there | have, can, this, be, the\n",
      "a | of, the, as, an, be\n",
      "or | are, usually, as, is, other\n",
      "to | for, that, was, the, had\n",
      "between | region, and, northern, southern, border\n",
      "is | has, of, and, as, for\n",
      "so | to, for, truly, that, if\n",
      "united | states, u, national, nations, us\n",
      "magazine | entitled, magazines, published, online, jerry\n",
      "proposed | theories, accepted, current, theory, advisory\n",
      "road | roads, hill, town, near, railway\n",
      "writers | novelists, fiction, poets, births, alumni\n",
      "older | age, household, children, than, family\n",
      "event | events, extinction, observed, celebrate, night\n",
      "consists | consisting, consist, each, composed, head\n",
      "frac | sqrt, equation, mbox, omega, cdot\n",
      "...\n",
      "\n",
      "Epoch: 3/5\n",
      "Loss:  2.347874402999878\n",
      "these | some, are, such, different, have\n",
      "new | york, london, press, at, on\n",
      "have | some, these, they, are, similar\n",
      "used | use, using, similar, can, are\n",
      "one | two, three, zero, five, four\n",
      "after | was, later, in, remained, had\n",
      "also | and, to, list, is, with\n",
      "of | the, an, in, a, by\n",
      "police | arrested, officers, trial, convicted, officer\n",
      "account | accounts, analysis, uncertainty, sample, edit\n",
      "marriage | divorce, marry, marriages, woman, sexual\n",
      "arts | martial, art, students, schools, school\n",
      "square | kilometers, quad, building, intersection, acres\n",
      "gold | silver, copper, bauxite, timber, bronze\n",
      "stage | broadway, actors, audiences, movies, performances\n",
      "proposed | theory, chemical, physics, field, theoretical\n",
      "...\n",
      "\n",
      "Epoch: 4/5\n",
      "Loss:  2.316225290298462\n",
      "states | united, state, of, union, kentucky\n",
      "as | and, of, some, the, in\n",
      "about | of, than, per, eight, from\n",
      "when | a, to, but, finally, they\n",
      "used | use, uses, or, types, common\n",
      "with | a, an, is, and, the\n",
      "where | called, in, and, a, is\n",
      "often | such, some, many, even, or\n",
      "egypt | egyptian, egyptians, bc, mediterranean, greeks\n",
      "powers | constitution, government, elected, supreme, sworn\n",
      "quite | because, it, very, easily, parts\n",
      "animals | humans, animal, human, eating, species\n",
      "shows | watching, episode, movies, comedy, television\n",
      "mainly | by, and, mostly, in, many\n",
      "accepted | consensus, rejected, their, writings, greeks\n",
      "police | crime, officials, arrested, officers, crimes\n",
      "...\n",
      "\n",
      "Epoch: 4/5\n",
      "Loss:  2.510927677154541\n",
      "i | t, you, am, know, don\n",
      "no | as, have, not, note, be\n",
      "called | is, are, can, either, a\n",
      "united | states, kingdom, canada, nations, state\n",
      "for | a, and, more, of, that\n",
      "people | have, culture, even, indigenous, suggest\n",
      "known | also, from, and, in, a\n",
      "they | their, the, but, not, have\n",
      "freedom | liberty, liberalism, capitalism, welfare, hayek\n",
      "troops | infantry, army, forces, soldiers, fighting\n",
      "recorded | album, records, late, songs, blues\n",
      "applied | used, disciplines, using, application, obtain\n",
      "hold | where, for, ever, a, be\n",
      "ice | hockey, frozen, melting, dry, skating\n",
      "file | files, windows, scripting, archiving, unix\n",
      "primarily | primary, newer, include, systems, provide\n",
      "...\n",
      "\n",
      "Epoch: 4/5\n",
      "Loss:  2.471025228500366\n",
      "s | by, the, in, nine, eight\n",
      "with | and, the, as, is, two\n",
      "also | as, of, the, from, in\n",
      "may | are, be, is, also, this\n",
      "d | b, l, j, k, one\n",
      "he | his, wife, himself, she, him\n",
      "these | are, not, of, to, have\n",
      "an | a, or, can, the, is\n",
      "shown | shows, show, as, numbers, a\n",
      "taking | soldiers, a, war, independent, tried\n",
      "assembly | elected, president, unicameral, elections, vote\n",
      "lived | was, began, he, birthplace, his\n",
      "universe | bang, cosmology, cosmic, matter, worlds\n",
      "scale | scales, large, systems, simulations, produces\n",
      "resources | natural, resource, comprehensive, management, arable\n",
      "numerous | many, several, including, early, some\n",
      "...\n",
      "\n",
      "Epoch: 4/5\n",
      "Loss:  2.6534345149993896\n",
      "more | the, most, this, than, they\n",
      "may | be, or, are, this, any\n",
      "such | or, used, most, generally, example\n",
      "world | us, nine, championship, game, america\n",
      "often | sometimes, as, some, usually, are\n",
      "most | and, of, in, more, this\n",
      "on | and, a, of, the, in\n",
      "as | the, and, of, in, also\n",
      "additional | each, increased, supply, not, identical\n",
      "except | where, than, number, some, since\n",
      "woman | her, female, she, children, marriage\n",
      "institute | university, research, universities, engineering, science\n",
      "egypt | egyptian, suez, israel, arab, mediterranean\n",
      "recorded | album, songs, recording, nights, concert\n",
      "hit | hits, song, album, airplay, chart\n",
      "http | www, html, links, org, web\n",
      "...\n",
      "\n",
      "Epoch: 4/5\n",
      "Loss:  2.396138906478882\n",
      "not | be, however, that, but, a\n",
      "had | to, his, the, who, were\n",
      "d | b, politician, eight, seven, nine\n",
      "is | as, for, the, a, be\n",
      "more | to, some, for, they, have\n",
      "eight | one, three, seven, five, six\n",
      "the | of, in, and, a, as\n",
      "or | are, a, from, have, usually\n",
      "centre | located, attractions, town, metropolitan, airport\n",
      "assembly | elected, elections, constitution, cabinet, unicameral\n",
      "units | unit, si, weights, metric, metre\n",
      "bbc | links, news, on, listing, day\n",
      "nobel | laureate, prize, recipient, physicist, physiology\n",
      "accepted | proposed, rejected, acceptance, view, not\n",
      "except | thus, not, all, eat, however\n",
      "square | kilometres, located, city, kilometers, km\n",
      "...\n",
      "\n",
      "Epoch: 4/5\n",
      "Loss:  2.2907676696777344\n",
      "his | he, him, was, brother, death\n",
      "will | not, must, allow, if, it\n",
      "used | use, using, uses, commonly, systems\n",
      "other | such, some, these, their, are\n",
      "b | d, politician, actor, composer, one\n",
      "three | two, five, one, four, zero\n",
      "all | a, every, on, and, their\n",
      "would | them, was, when, to, could\n",
      "san | francisco, puerto, los, jose, angeles\n",
      "file | files, format, windows, xml, formats\n",
      "bible | scripture, tanakh, testament, hebrew, biblical\n",
      "numerous | by, many, kept, few, including\n",
      "award | awards, awarded, best, academy, nominated\n",
      "universe | creator, cosmology, galaxies, beings, explained\n",
      "arts | art, martial, practitioners, students, style\n",
      "operations | operation, security, such, systems, strategic\n",
      "...\n",
      "\n",
      "Epoch: 5/5\n",
      "Loss:  2.2447259426116943\n",
      "were | had, was, many, for, in\n",
      "b | d, politician, composer, actor, laureate\n",
      "i | iii, here, my, ii, me\n",
      "on | is, a, the, in, with\n",
      "from | the, in, of, and, a\n",
      "nine | four, two, zero, one, three\n",
      "known | name, a, from, called, also\n",
      "all | are, or, other, being, many\n",
      "orthodox | catholic, coptic, denominations, church, christian\n",
      "instance | not, be, or, arbitrary, any\n",
      "numerous | many, including, several, among, famous\n",
      "defense | guard, attack, corps, missile, defence\n",
      "writers | novelists, poets, fiction, alumni, playwrights\n",
      "engineering | mechanical, engineers, electronics, technology, systems\n",
      "assembly | elections, elected, constitution, seats, parliamentary\n",
      "award | awards, awarded, best, academy, oscars\n",
      "...\n",
      "\n",
      "Epoch: 5/5\n",
      "Loss:  1.7058442831039429\n",
      "with | and, the, a, to, into\n",
      "d | l, b, c, de, m\n",
      "about | it, this, of, and, very\n",
      "s | and, in, the, a, as\n",
      "than | more, and, the, with, which\n",
      "been | suggest, some, it, had, has\n",
      "into | the, and, with, a, in\n",
      "their | the, they, to, even, with\n",
      "additional | increased, making, then, reduce, previous\n",
      "experience | mental, subjective, psychological, experiences, pain\n",
      "question | answer, whether, be, might, problem\n",
      "bill | johnson, jimmy, harry, bills, jones\n",
      "mainly | mostly, largely, developed, including, systems\n",
      "applied | referred, applying, is, use, calculus\n",
      "report | reports, commission, committee, interview, agency\n",
      "units | unit, measured, conversion, measurement, si\n",
      "...\n",
      "\n",
      "Epoch: 5/5\n",
      "Loss:  2.3843283653259277\n",
      "such | also, or, are, form, often\n",
      "often | or, a, as, and, which\n",
      "some | are, many, these, has, those\n",
      "on | in, the, at, and, for\n",
      "their | they, to, and, for, also\n",
      "by | the, of, in, one, a\n",
      "i | t, you, we, me, said\n",
      "that | a, it, or, to, the\n",
      "question | our, whether, we, answer, but\n",
      "gold | silver, metals, bronze, timber, coin\n",
      "joseph | smith, john, jr, politician, nicholas\n",
      "dr | j, robert, carl, scientist, edited\n",
      "applications | systems, application, computer, use, require\n",
      "construction | constructed, transportation, scale, wooden, building\n",
      "mainly | mostly, east, north, northern, largely\n",
      "older | versus, household, age, living, median\n",
      "...\n",
      "\n",
      "Epoch: 5/5\n",
      "Loss:  2.239135503768921\n",
      "was | had, after, by, the, his\n",
      "when | it, a, so, the, to\n",
      "eight | one, six, four, seven, nine\n",
      "had | was, his, after, the, before\n",
      "would | not, was, to, able, be\n",
      "for | and, the, of, as, since\n",
      "a | the, of, and, in, is\n",
      "american | actor, actress, brazilian, canadian, musician\n",
      "smith | john, joseph, taylor, actor, politician\n",
      "liberal | party, liberals, conservative, opposition, political\n",
      "quite | has, usage, which, not, other\n",
      "road | roads, traffic, lane, routes, highway\n",
      "placed | placing, capture, side, throw, uis\n",
      "nobel | prize, laureate, recipient, physiology, physicist\n",
      "heavy | amounts, damaging, due, other, heavier\n",
      "know | we, you, do, didn, anything\n",
      "...\n",
      "\n",
      "Epoch: 5/5\n",
      "Loss:  2.377938747406006\n",
      "th | century, centuries, rd, nd, six\n",
      "two | zero, four, five, three, one\n",
      "three | four, seven, six, five, zero\n",
      "american | b, actress, actor, nine, musician\n",
      "was | in, the, and, of, s\n",
      "into | the, with, as, which, in\n",
      "however | have, has, had, of, and\n",
      "d | b, seven, eight, politician, nine\n",
      "gold | silver, metals, zinc, bronze, timber\n",
      "animals | animal, mammals, wild, insects, species\n",
      "shows | movies, television, shown, sequence, different\n",
      "troops | army, soldiers, war, killed, allied\n",
      "arts | martial, academy, school, art, aikido\n",
      "mean | means, value, can, calculate, or\n",
      "police | guard, department, crimes, criminal, officers\n",
      "alternative | complementary, learning, e, as, uses\n",
      "...\n",
      "\n",
      "Epoch: 5/5\n",
      "Loss:  2.5978429317474365\n",
      "state | states, county, borders, counties, michigan\n",
      "over | under, years, to, total, and\n",
      "during | the, and, in, period, on\n",
      "nine | one, four, three, seven, eight\n",
      "or | are, a, some, can, other\n",
      "was | were, the, later, became, a\n",
      "years | year, female, male, age, zero\n",
      "often | are, some, use, usually, many\n",
      "award | awards, awarded, emmy, best, academy\n",
      "existence | being, contradiction, inferred, god, belief\n",
      "pressure | exhaust, pump, temperature, liquid, heating\n",
      "professional | hockey, tennis, football, wrestler, playing\n",
      "engine | engines, combustion, powered, piston, steam\n",
      "issue | asserted, accept, constitution, admitted, declaring\n",
      "liberal | liberals, conservatives, conservatism, liberalism, party\n",
      "hold | question, viewpoints, believe, person, fundamentalists\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Get our noise distribution\n",
    "# Using word frequencies calculated earlier in the notebook\n",
    "word_freqs = np.array(sorted(freqs.values(), reverse=True))\n",
    "unigram_dist = word_freqs/word_freqs.sum()\n",
    "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
    "\n",
    "# instantiating the model\n",
    "embedding_dim = 300\n",
    "model = SkipGramNeg(len(vocab_to_int), embedding_dim, noise_dist=noise_dist).to(device)\n",
    "\n",
    "# using the loss that we defined\n",
    "criterion = NegativeSamplingLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print_every = 1500\n",
    "steps = 0\n",
    "epochs = 5\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # get our input, target batches\n",
    "    for input_words, target_words in get_batches(train_words, 512):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # input, outpt, and noise vectors\n",
    "        input_vectors = model.forward_input(inputs)\n",
    "        output_vectors = model.forward_output(targets)\n",
    "        noise_vectors = model.forward_noise(inputs.shape[0], 5)\n",
    "\n",
    "        # negative sampling loss\n",
    "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs))\n",
    "            print(\"Loss: \", loss.item()) # avg batch loss at this point in training\n",
    "            valid_examples, valid_similarities = cosine_similarity(model.in_embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6)\n",
    "\n",
    "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
    "            print(\"...\\n\")"
   ]
  }
 ]
}