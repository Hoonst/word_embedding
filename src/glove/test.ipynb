{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('cs224w')",
   "metadata": {
    "interpreter": {
     "hash": "11fe729db4f79aed458297f4cec3c3629ef4e04793785c93ae28376e0154d3b6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# of words: 100000\nVocabulary length: 12024\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GloveDataSetting:    \n",
    "    def __init__(self, text, n_words=200000, window_size=5):\n",
    "        self._window_size = window_size\n",
    "        self._tokens = text.split(\" \")[:n_words]\n",
    "        word_counter = Counter()\n",
    "        word_counter.update(self._tokens)\n",
    "        self._word2id = {w:i for i, (w,_) in enumerate(word_counter.most_common())}\n",
    "        self._id2word = {i:w for w, i in self._word2id.items()}\n",
    "        self._vocab_len = len(self._word2id)\n",
    "\n",
    "        self._id_tokens = [self._word2id[w] for w in self._tokens]\n",
    "\n",
    "        self._create_coocurrence_matrix()\n",
    "\n",
    "        print(\"# of words: {}\".format(len(self._tokens)))\n",
    "        print(\"Vocabulary length: {}\".format(self._vocab_len))\n",
    "\n",
    "    def _create_coocurrence_matrix(self):\n",
    "        cooc_mat = defaultdict(Counter)\n",
    "        for i, w in enumerate(self._id_tokens):\n",
    "            start_i = max(i - self._window_size, 0)\n",
    "            end_i = min(i + self._window_size + 1, len(self._id_tokens))\n",
    "            for j in range(start_i, end_i):\n",
    "                if i != j:\n",
    "                    c = self._id_tokens[j]\n",
    "                    cooc_mat[w][c] += 1 / abs(j-i)\n",
    "\n",
    "        self._i_idx = list()\n",
    "        self._j_idx = list()\n",
    "        self._xij = list()\n",
    "\n",
    "        #Create indexes and x values tensors\n",
    "        for w, cnt in cooc_mat.items():\n",
    "            for c, v in cnt.items():\n",
    "                self._i_idx.append(w)\n",
    "                self._j_idx.append(c)\n",
    "                self._xij.append(v)\n",
    "\n",
    "        self._i_idx = torch.LongTensor(self._i_idx).cuda()\n",
    "        self._j_idx = torch.LongTensor(self._j_idx).cuda()\n",
    "        self._xij = torch.FloatTensor(self._xij).cuda()\n",
    "        # _xij는 빈도\n",
    "        # _i_idx / _j_idx 는 index\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "        #Generate random idx\n",
    "        rand_ids = torch.LongTensor(np.random.choice(len(self._xij), len(self._xij), replace=False))\n",
    "\n",
    "        for p in range(0, len(rand_ids), batch_size):\n",
    "            batch_ids = rand_ids[p:p+batch_size]\n",
    "            yield self._xij[batch_ids], self._i_idx[batch_ids], self._j_idx[batch_ids]\n",
    "\n",
    "class GloveDataset(Dataset):\n",
    "    def __init__(self, text, n_words=200000, window_size=5):\n",
    "        self._window_size = window_size\n",
    "        self._tokens = text.split(\" \")[:n_words]\n",
    "        word_counter = Counter()\n",
    "        word_counter.update(self._tokens)\n",
    "        self._word2id = {w:i for i, (w,_) in enumerate(word_counter.most_common())}\n",
    "        self._id2word = {i:w for w, i in self._word2id.items()}\n",
    "        self._vocab_len = len(self._word2id)\n",
    "\n",
    "        self._id_tokens = [self._word2id[w] for w in self._tokens]\n",
    "\n",
    "        self._create_coocurrence_matrix()\n",
    "\n",
    "        print(\"# of words: {}\".format(len(self._tokens)))\n",
    "        print(\"Vocabulary length: {}\".format(self._vocab_len))\n",
    "\n",
    "    def _create_coocurrence_matrix(self):\n",
    "        cooc_mat = defaultdict(Counter)\n",
    "        for i, w in enumerate(self._id_tokens):\n",
    "            start_i = max(i - self._window_size, 0)\n",
    "            end_i = min(i + self._window_size + 1, len(self._id_tokens))\n",
    "            for j in range(start_i, end_i):\n",
    "                if i != j:\n",
    "                    c = self._id_tokens[j]\n",
    "                    # abs로 나누는 이유는 아마 거리에 따른 영향력을 반영하려는 것이 아닐까 싶다.\n",
    "                    cooc_mat[w][c] += 1 / abs(j-i)\n",
    "\n",
    "        self._i_idx = list()\n",
    "        self._j_idx = list()\n",
    "        self._xij = list()\n",
    "\n",
    "        #Create indexes and x values tensors\n",
    "        for w, cnt in cooc_mat.items():\n",
    "            for c, v in cnt.items():\n",
    "                self._i_idx.append(w)\n",
    "                self._j_idx.append(c)\n",
    "                self._xij.append(v)\n",
    "\n",
    "        self._i_idx = torch.LongTensor(self._i_idx).cuda()\n",
    "        self._j_idx = torch.LongTensor(self._j_idx).cuda()\n",
    "        self._xij = torch.FloatTensor(self._xij).cuda()\n",
    "        # _xij는 빈도\n",
    "        # _i_idx / _j_idx 는 index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._i_idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self._xij[idx], self._i_idx[idx], self._j_idx[idx]\n",
    "\n",
    "\n",
    "dataset = GloveDataset(open(\"data/text8\").read(), 100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 [tensor([1.0000, 1.0000, 0.2500, 1.0000], device='cuda:0'), tensor([  33,    4, 7430,  137], device='cuda:0'), tensor([8049, 7370, 7429, 1873], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sampled_batched in enumerate(dataloader):\n",
    "    print(i_batch, sampled_batched)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}